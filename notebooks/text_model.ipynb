{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2103151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9283240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreeLearningModel:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(1, 3),\n",
    "            max_features=1000\n",
    "        )\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.similarity_model = None\n",
    "        self.cree_to_english = defaultdict(list)\n",
    "        self.english_to_cree = defaultdict(list)\n",
    "        self.cree_embeddings = None\n",
    "        self.english_embeddings = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de6b1b",
   "metadata": {},
   "source": [
    "* TF-IDF vectorizer \n",
    "    - to convert Cree and English words into numeric vectors\n",
    "    - Uses character-level n-grams (1 to 3 characters) to capture sub-word patterns, which is helpful for handling spelling variations and morphological structures.\n",
    "    - Limit to top 1000 character n-grams for efficiency\n",
    "* MultiLabelBinarizer\n",
    "    - used to encode multiple possible English translations for a single Cree word into a binary format suitable for multi-label classification.\n",
    "* similarity_model \n",
    "    - similarity model used to find the closest English match to a given Cree word.\n",
    "* cree_to_english and english_to_cree\n",
    "    - used to convert between Cree and English words, using the similarity model to find the closest match.\n",
    "* cree_embeddings and english_embeddings\n",
    "    - store the numeric TF-IDF vector representations (embeddings) for all Cree and English words, to be used in similarity comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143588e",
   "metadata": {},
   "source": [
    "**Preprocessing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        Preprocess the Cree-English dataset\n",
    "        \"\"\"\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        print(f\"Original dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Clean the data\n",
    "        df['Cree'] = df['Cree'].str.strip()\n",
    "        df['English'] = df['English'].str.strip()\n",
    "        \n",
    "        # Remove any empty rows\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Create mappings\n",
    "        for _, row in df.iterrows():\n",
    "            cree_word = row['Cree'].lower()\n",
    "            english_meaning = row['English'].lower()\n",
    "            \n",
    "            if english_meaning not in self.cree_to_english[cree_word]:\n",
    "                # a Cree word maps to a list of English meanings\n",
    "                self.cree_to_english[cree_word].append(english_meaning)\n",
    "            \n",
    "            if cree_word not in self.english_to_cree[english_meaning]:\n",
    "                # an English word maps to a list of Cree words\n",
    "                self.english_to_cree[english_meaning].append(cree_word)\n",
    "        \n",
    "        print(f\"Unique Cree words: {len(self.cree_to_english)}\")\n",
    "        print(f\"Unique English meanings: {len(self.english_to_cree)}\")\n",
    "        \n",
    "        # Analyze one-to-many mappings\n",
    "        multi_meaning_cree = {k: v for k, v in self.cree_to_english.items() if len(v) > 1}\n",
    "        print(f\"Cree words with multiple meanings: {len(multi_meaning_cree)}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(\"\\nExamples of Cree words with multiple meanings:\")\n",
    "        for i, (cree, meanings) in enumerate(list(multi_meaning_cree.items())[:5]):\n",
    "            print(f\"  {cree}: {', '.join(meanings)}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.preprocess_data = preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46021132",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CreeLearningModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ebbc722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Original dataset shape: (1188, 2)\n",
      "Unique Cree words: 999\n",
      "Unique English meanings: 1099\n",
      "Cree words with multiple meanings: 147\n",
      "\n",
      "Examples of Cree words with multiple meanings:\n",
      "  achiwpayin: it shrunk, it shrinks\n",
      "  ahin: put, place me\n",
      "  ahp≈ç: or, either\n",
      "  akihtƒÅsowin: a number, a figure\n",
      "  akihtƒÅs≈çna: numbers, figures\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'D:\\.SeaThru\\Work\\AI-for-Indigenous-Language--Revitalization-in-Canada\\AI-for-Indigenous-Language-Revitalization-in-Canada\\data\\cleaned\\cree_english_text_only.csv'  # Update this path\n",
    "df = model.preprocess_data(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd5740b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cree</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acahkosak</td>\n",
       "      <td>stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>achimƒìwak</td>\n",
       "      <td>they are telling a story about him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>achimoh</td>\n",
       "      <td>tell a story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>achimostamaw√¢w</td>\n",
       "      <td>a story is told to him/her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>achimostaw</td>\n",
       "      <td>tell him a story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Cree                             English\n",
       "0       acahkosak                               stars\n",
       "1       achimƒìwak  they are telling a story about him\n",
       "2         achimoh                        tell a story\n",
       "3  achimostamaw√¢w          a story is told to him/her\n",
       "4      achimostaw                    tell him a story"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7973554",
   "metadata": {},
   "source": [
    "**Creating Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82aca322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, df):\n",
    "        \"\"\"\n",
    "        Create TF-IDF embeddings for Cree words and English meanings\n",
    "        \"\"\"\n",
    "        print(\"\\nCreating embeddings...\")\n",
    "        \n",
    "        # Get unique words and meanings\n",
    "        unique_cree = list(self.cree_to_english.keys())\n",
    "        unique_english = list(self.english_to_cree.keys())\n",
    "        \n",
    "        # Create embeddings for Cree words\n",
    "        self.cree_embeddings = self.vectorizer.fit_transform(unique_cree)\n",
    "        \n",
    "        # Create embeddings for English meanings (using same vectorizer)\n",
    "        self.english_embeddings = self.vectorizer.transform(unique_english)\n",
    "        \n",
    "        print(f\"Cree embeddings shape: {self.cree_embeddings.shape}\")\n",
    "        print(f\"English embeddings shape: {self.english_embeddings.shape}\")\n",
    "        \n",
    "        return unique_cree, unique_english\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.create_embeddings = create_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7629649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating embeddings...\n",
      "Cree embeddings shape: (999, 1000)\n",
      "English embeddings shape: (1099, 1000)\n"
     ]
    }
   ],
   "source": [
    "unique_cree, unique_english = model.create_embeddings(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec51ce",
   "metadata": {},
   "source": [
    "**Building Similarity Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51a419ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_model(self):\n",
    "        \"\"\"\n",
    "        Build a similarity-based model for translation\n",
    "        \"\"\"\n",
    "        print(\"\\nBuilding similarity model...\")\n",
    "        \n",
    "        # Calculate similarity matrix between Cree and English\n",
    "        self.similarity_matrix = cosine_similarity(self.cree_embeddings, self.english_embeddings)\n",
    "        print(f\"Similarity matrix shape: {self.similarity_matrix.shape}\")\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.build_similarity_model = build_similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f0e032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building similarity model...\n",
      "Similarity matrix shape: (999, 1099)\n"
     ]
    }
   ],
   "source": [
    "model.build_similarity_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c3f9b",
   "metadata": {},
   "source": [
    "**Finding Translations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc01bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_translations(self, cree_word, top_k=5):\n",
    "        \"\"\"\n",
    "        Find top-k English translations for a Cree word\n",
    "        \"\"\"\n",
    "        cree_word = cree_word.lower().strip()\n",
    "        \n",
    "        # Direct lookup first\n",
    "        if cree_word in self.cree_to_english:\n",
    "            return self.cree_to_english[cree_word]\n",
    "        \n",
    "        # If not found, use similarity\n",
    "        unique_cree = list(self.cree_to_english.keys())\n",
    "        unique_english = list(self.english_to_cree.keys())\n",
    "        \n",
    "        if cree_word not in unique_cree:\n",
    "            # Find most similar Cree word\n",
    "            query_embedding = self.vectorizer.transform([cree_word])\n",
    "            similarities = cosine_similarity(query_embedding, self.cree_embeddings)[0]\n",
    "            \n",
    "            # Get top similar Cree words\n",
    "            top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "            similar_cree_words = [unique_cree[i] for i in top_indices if similarities[i] > 0.1]\n",
    "            \n",
    "            # Get their translations\n",
    "            translations = []\n",
    "            for similar_word in similar_cree_words:\n",
    "                translations.extend(self.cree_to_english[similar_word])\n",
    "            \n",
    "            return list(set(translations))[:top_k]\n",
    "        \n",
    "        return []\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.find_translations = find_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d86d2d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing the Model ===\n",
      "'ahƒìw' -> ['he placed him']\n",
      "'ahin' -> ['put', 'place me']\n",
      "'ahp≈ç' -> ['or', 'either']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Testing the Model ===\")\n",
    "        \n",
    "# Test translation lookup\n",
    "test_words = ['ahƒìw', 'ahin', 'ahp≈ç']\n",
    "for word in test_words:\n",
    "    translations = model.find_translations(word)\n",
    "    print(f\"'{word}' -> {translations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620befd",
   "metadata": {},
   "source": [
    "**Finding Cree Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452536d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cree_words(self, english_meaning, top_k=5):\n",
    "        \"\"\"\n",
    "        Find Cree words for an English meaning\n",
    "        \"\"\"\n",
    "        english_meaning = english_meaning.lower().strip()\n",
    "        \n",
    "        # Direct lookup first\n",
    "        if english_meaning in self.english_to_cree:\n",
    "            return self.english_to_cree[english_meaning]\n",
    "        \n",
    "        # If not found, use similarity\n",
    "        unique_english = list(self.english_to_cree.keys())\n",
    "        \n",
    "        if english_meaning not in unique_english:\n",
    "            # Find most similar English meaning\n",
    "            query_embedding = self.vectorizer.transform([english_meaning])\n",
    "            similarities = cosine_similarity(query_embedding, self.english_embeddings)[0]\n",
    "            \n",
    "            # Get top similar English meanings\n",
    "            top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "            similar_meanings = [unique_english[i] for i in top_indices if similarities[i] > 0.1]\n",
    "            \n",
    "            # Get their Cree translations\n",
    "            cree_words = []\n",
    "            for similar_meaning in similar_meanings:\n",
    "                cree_words.extend(self.english_to_cree[similar_meaning])\n",
    "            \n",
    "            return list(set(cree_words))[:top_k]\n",
    "        \n",
    "        return []\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.find_cree_words = find_cree_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "494ce5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reverse lookup:\n",
      "'place him' -> ['ahih']\n",
      "'or' -> ['ahp≈ç']\n",
      "'this' -> ['≈çma']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReverse lookup:\")\n",
    "test_meanings = ['place him', 'or', 'this']\n",
    "for meaning in test_meanings:\n",
    "    cree_words = model.find_cree_words(meaning)\n",
    "    print(f\"'{meaning}' -> {cree_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda86308",
   "metadata": {},
   "source": [
    "**Creating Learning Exercises**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d8e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_exercises(self, difficulty='mixed'):\n",
    "        \"\"\"\n",
    "        Create learning exercises based on the dataset\n",
    "        \"\"\"\n",
    "        exercises = []\n",
    "        \n",
    "        if difficulty == 'easy':\n",
    "            # Single meaning words only\n",
    "            words = {k: v for k, v in self.cree_to_english.items() if len(v) == 1}\n",
    "        elif difficulty == 'hard':\n",
    "            # Multiple meaning words only\n",
    "            words = {k: v for k, v in self.cree_to_english.items() if len(v) > 1}\n",
    "        else:\n",
    "            # Mixed difficulty\n",
    "            words = self.cree_to_english\n",
    "        \n",
    "        # Multiple choice exercises\n",
    "        word_list = list(words.keys())\n",
    "        np.random.shuffle(word_list)\n",
    "        \n",
    "        for cree_word in word_list[:20]:  # Create 20 exercises\n",
    "            correct_answers = words[cree_word]\n",
    "            \n",
    "            # Get wrong answers\n",
    "            all_meanings = list(self.english_to_cree.keys())\n",
    "            wrong_answers = [m for m in all_meanings if m not in correct_answers]\n",
    "            wrong_choices = np.random.choice(wrong_answers, min(3, len(wrong_answers)), replace=False)\n",
    "            \n",
    "            # Create multiple choice\n",
    "            choices = list(correct_answers) + list(wrong_choices)\n",
    "            np.random.shuffle(choices)\n",
    "            \n",
    "            exercises.append({\n",
    "                'cree_word': cree_word,\n",
    "                'choices': choices,\n",
    "                'correct_answers': correct_answers,\n",
    "                'type': 'multiple_choice'\n",
    "            })\n",
    "        \n",
    "        return exercises\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.create_learning_exercises = create_learning_exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3bca9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Learning Exercises ===\n",
      "\n",
      "Exercise 1:\n",
      "What does 'p≈çsik' mean?\n",
      "  1. hurry up\n",
      "  2. a number\n",
      "  3. laugh\n",
      "  4. all of you get on board\n",
      "Correct answer(s): ['all of you get on board']\n",
      "\n",
      "Exercise 2:\n",
      "What does 'kakwƒìcihkƒìm≈çwin' mean?\n",
      "  1. inquiring\n",
      "  2. treat him with respect\n",
      "  3. asking\n",
      "  4. rocks\n",
      "  5. redirectional response\n",
      "Correct answer(s): ['asking', 'inquiring']\n",
      "\n",
      "Exercise 3:\n",
      "What does 'ponihtƒÅ' mean?\n",
      "  1. a forest\n",
      "  2. leave it alone\n",
      "  3. this is a particle towards\n",
      "  4. put\n",
      "Correct answer(s): ['leave it alone']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Sample Learning Exercises ===\")\n",
    "exercises = model.create_learning_exercises(difficulty='mixed')\n",
    "        \n",
    "for i, exercise in enumerate(exercises[:3]):  # Show first 3 exercises\n",
    "    print(f\"\\nExercise {i+1}:\")\n",
    "    print(f\"What does '{exercise['cree_word']}' mean?\")\n",
    "    for j, choice in enumerate(exercise['choices'], 1):\n",
    "        print(f\"  {j}. {choice}\")\n",
    "    print(f\"Correct answer(s): {exercise['correct_answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2ed3d",
   "metadata": {},
   "source": [
    "**Saving Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a3f76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self, filepath='cree_model.pkl'):\n",
    "        \"\"\"\n",
    "        Save the trained model\n",
    "        \"\"\"\n",
    "        model_data = {\n",
    "            'vectorizer': self.vectorizer,\n",
    "            'cree_to_english': dict(self.cree_to_english),\n",
    "            'english_to_cree': dict(self.english_to_cree),\n",
    "            'similarity_matrix': self.similarity_matrix if hasattr(self, 'similarity_matrix') else None\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.save_model = save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b70882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/cree_learning_model.pkl\n"
     ]
    }
   ],
   "source": [
    "model.save_model('../models/cree_learning_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c125ac",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3325cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(self, filepath='cree_model.pkl'):\n",
    "        \"\"\"\n",
    "        Load a saved model\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        self.vectorizer = model_data['vectorizer']\n",
    "        self.cree_to_english = defaultdict(list, model_data['cree_to_english'])\n",
    "        self.english_to_cree = defaultdict(list, model_data['english_to_cree'])\n",
    "        self.similarity_matrix = model_data.get('similarity_matrix')\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.load_model = load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a7752",
   "metadata": {},
   "source": [
    "**Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dfca8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(self, test_ratio=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Comprehensive model evaluation with multiple metrics\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Model Evaluation ===\")\n",
    "        \n",
    "        # Prepare data for evaluation\n",
    "        all_cree_words = list(self.cree_to_english.keys())\n",
    "        \n",
    "        # Split data for evaluation\n",
    "        train_words, test_words = train_test_split(\n",
    "            all_cree_words, \n",
    "            test_size=test_ratio, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"Training words: {len(train_words)}\")\n",
    "        print(f\"Testing words: {len(test_words)}\")\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        evaluation_results = {\n",
    "            'exact_match_accuracy': 0,\n",
    "            'partial_match_accuracy': 0,\n",
    "            'top_3_accuracy': 0,\n",
    "            'top_5_accuracy': 0,\n",
    "            'average_similarity_score': 0,\n",
    "            'coverage_score': 0,\n",
    "            'multilingual_precision': 0,\n",
    "            'multilingual_recall': 0,\n",
    "            'multilingual_f1': 0\n",
    "        }\n",
    "        \n",
    "        exact_matches = 0\n",
    "        partial_matches = 0\n",
    "        top_3_matches = 0\n",
    "        top_5_matches = 0\n",
    "        similarity_scores = []\n",
    "        covered_words = 0\n",
    "        \n",
    "        # Detailed results for analysis\n",
    "        detailed_results = []\n",
    "        \n",
    "        for cree_word in test_words:\n",
    "            true_meanings = set(self.cree_to_english[cree_word])\n",
    "            predicted_meanings = self.find_translations(cree_word, top_k=5)\n",
    "            predicted_set = set(predicted_meanings)\n",
    "            \n",
    "            # Exact match (all meanings predicted correctly)\n",
    "            if true_meanings == predicted_set:\n",
    "                exact_matches += 1\n",
    "            \n",
    "            # Partial match (at least one meaning predicted correctly)\n",
    "            if len(true_meanings.intersection(predicted_set)) > 0:\n",
    "                partial_matches += 1\n",
    "            \n",
    "            # Top-k accuracy\n",
    "            if len(true_meanings.intersection(set(predicted_meanings[:3]))) > 0:\n",
    "                top_3_matches += 1\n",
    "            if len(true_meanings.intersection(set(predicted_meanings[:5]))) > 0:\n",
    "                top_5_matches += 1\n",
    "            \n",
    "            # Coverage (model found the word)\n",
    "            if len(predicted_meanings) > 0:\n",
    "                covered_words += 1\n",
    "            \n",
    "            # Similarity score\n",
    "            if len(predicted_meanings) > 0:\n",
    "                # Calculate Jaccard similarity\n",
    "                jaccard_sim = len(true_meanings.intersection(predicted_set)) / len(true_meanings.union(predicted_set))\n",
    "                similarity_scores.append(jaccard_sim)\n",
    "            else:\n",
    "                similarity_scores.append(0)\n",
    "            \n",
    "            # Store detailed results\n",
    "            detailed_results.append({\n",
    "                'cree_word': cree_word,\n",
    "                'true_meanings': list(true_meanings),\n",
    "                'predicted_meanings': predicted_meanings,\n",
    "                'exact_match': true_meanings == predicted_set,\n",
    "                'partial_match': len(true_meanings.intersection(predicted_set)) > 0,\n",
    "                'jaccard_similarity': similarity_scores[-1]\n",
    "            })\n",
    "        \n",
    "        # Calculate final scores\n",
    "        n_test = len(test_words)\n",
    "        evaluation_results['exact_match_accuracy'] = exact_matches / n_test\n",
    "        evaluation_results['partial_match_accuracy'] = partial_matches / n_test\n",
    "        evaluation_results['top_3_accuracy'] = top_3_matches / n_test\n",
    "        evaluation_results['top_5_accuracy'] = top_5_matches / n_test\n",
    "        evaluation_results['average_similarity_score'] = np.mean(similarity_scores)\n",
    "        evaluation_results['coverage_score'] = covered_words / n_test\n",
    "        \n",
    "        # Multi-label classification metrics\n",
    "        y_true_multilabel = []\n",
    "        y_pred_multilabel = []\n",
    "        \n",
    "        # Get all possible English meanings for encoding\n",
    "        all_english_meanings = list(self.english_to_cree.keys())\n",
    "        \n",
    "        for result in detailed_results:\n",
    "            # True labels (binary vector)\n",
    "            true_vector = [1 if meaning in result['true_meanings'] else 0 for meaning in all_english_meanings]\n",
    "            y_true_multilabel.append(true_vector)\n",
    "            \n",
    "            # Predicted labels (binary vector)\n",
    "            pred_vector = [1 if meaning in result['predicted_meanings'] else 0 for meaning in all_english_meanings]\n",
    "            y_pred_multilabel.append(pred_vector)\n",
    "        \n",
    "        # Calculate precision, recall, F1 for multi-label scenario\n",
    "        if len(y_true_multilabel) > 0:\n",
    "            evaluation_results['multilingual_precision'] = precision_score(\n",
    "                y_true_multilabel, y_pred_multilabel, average='micro', zero_division=0\n",
    "            )\n",
    "            evaluation_results['multilingual_recall'] = recall_score(\n",
    "                y_true_multilabel, y_pred_multilabel, average='micro', zero_division=0\n",
    "            )\n",
    "            evaluation_results['multilingual_f1'] = f1_score(\n",
    "                y_true_multilabel, y_pred_multilabel, average='micro', zero_division=0\n",
    "            )\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        print(f\"Exact Match Accuracy: {evaluation_results['exact_match_accuracy']:.3f}\")\n",
    "        print(f\"Partial Match Accuracy: {evaluation_results['partial_match_accuracy']:.3f}\")\n",
    "        print(f\"Top-3 Accuracy: {evaluation_results['top_3_accuracy']:.3f}\")\n",
    "        print(f\"Top-5 Accuracy: {evaluation_results['top_5_accuracy']:.3f}\")\n",
    "        print(f\"Average Similarity Score: {evaluation_results['average_similarity_score']:.3f}\")\n",
    "        print(f\"Coverage Score: {evaluation_results['coverage_score']:.3f}\")\n",
    "        print(f\"Multi-label Precision: {evaluation_results['multilingual_precision']:.3f}\")\n",
    "        print(f\"Multi-label Recall: {evaluation_results['multilingual_recall']:.3f}\")\n",
    "        print(f\"Multi-label F1-Score: {evaluation_results['multilingual_f1']:.3f}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(\"\\n--- Sample Predictions ---\")\n",
    "        for i in range(min(5, len(detailed_results))):\n",
    "            result = detailed_results[i]\n",
    "            print(f\"\\nCree: '{result['cree_word']}'\")\n",
    "            print(f\"  True: {result['true_meanings']}\")\n",
    "            print(f\"  Predicted: {result['predicted_meanings']}\")\n",
    "            print(f\"  Exact Match: {result['exact_match']}\")\n",
    "            print(f\"  Similarity: {result['jaccard_similarity']:.3f}\")\n",
    "        \n",
    "        # Error analysis\n",
    "        print(\"\\n--- Error Analysis ---\")\n",
    "        failed_predictions = [r for r in detailed_results if not r['partial_match']]\n",
    "        print(f\"Failed predictions: {len(failed_predictions)}\")\n",
    "        \n",
    "        if failed_predictions:\n",
    "            print(\"Examples of failed predictions:\")\n",
    "            for i in range(min(3, len(failed_predictions))):\n",
    "                result = failed_predictions[i]\n",
    "                print(f\"  '{result['cree_word']}': {result['true_meanings']} -> {result['predicted_meanings']}\")\n",
    "        \n",
    "        return evaluation_results, detailed_results\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.evaluate_model = evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82d3e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Evaluation ===\n",
      "Training words: 799\n",
      "Testing words: 200\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Exact Match Accuracy: 1.000\n",
      "Partial Match Accuracy: 1.000\n",
      "Top-3 Accuracy: 1.000\n",
      "Top-5 Accuracy: 1.000\n",
      "Average Similarity Score: 1.000\n",
      "Coverage Score: 1.000\n",
      "Multi-label Precision: 1.000\n",
      "Multi-label Recall: 1.000\n",
      "Multi-label F1-Score: 1.000\n",
      "\n",
      "--- Sample Predictions ---\n",
      "\n",
      "Cree: 'kƒÅyƒÅn'\n",
      "  True: ['you can have it']\n",
      "  Predicted: ['you can have it']\n",
      "  Exact Match: True\n",
      "  Similarity: 1.000\n",
      "\n",
      "Cree: '≈çskƒìsƒìh≈çwin'\n",
      "  True: ['wearing new clothes']\n",
      "  Predicted: ['wearing new clothes']\n",
      "  Exact Match: True\n",
      "  Similarity: 1.000\n",
      "\n",
      "Cree: 'ƒÅkosimowin'\n",
      "  True: ['nestling']\n",
      "  Predicted: ['nestling']\n",
      "  Exact Match: True\n",
      "  Similarity: 1.000\n",
      "\n",
      "Cree: 'ihtakon'\n",
      "  True: ['it exists']\n",
      "  Predicted: ['it exists']\n",
      "  Exact Match: True\n",
      "  Similarity: 1.000\n",
      "\n",
      "Cree: 'nisitohta'\n",
      "  True: ['understand it']\n",
      "  Predicted: ['understand it']\n",
      "  Exact Match: True\n",
      "  Similarity: 1.000\n",
      "\n",
      "--- Error Analysis ---\n",
      "Failed predictions: 0\n"
     ]
    }
   ],
   "source": [
    "eval_results, detailed_results = model.evaluate_model(test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd2ce305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Model Performance Summary ===\n",
      "üìä Overall Accuracy: 100.0%\n",
      "üéØ Exact Match Rate: 100.0%\n",
      "üìà Top-3 Accuracy: 100.0%\n",
      "üîç Coverage: 100.0%\n",
      "‚ö° F1-Score: 1.000\n",
      "üìè Avg Similarity: 1.000\n",
      "\n",
      "=== Model Quality Interpretation ===\n",
      "‚úÖ EXCELLENT: Model performs very well!\n",
      "\n",
      "=== Recommendations ===\n",
      "Model is performing well. No recommendations provided.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Model Performance Summary ===\")\n",
    "print(f\"üìä Overall Accuracy: {eval_results['partial_match_accuracy']:.1%}\")\n",
    "print(f\"üéØ Exact Match Rate: {eval_results['exact_match_accuracy']:.1%}\")\n",
    "print(f\"üìà Top-3 Accuracy: {eval_results['top_3_accuracy']:.1%}\")\n",
    "print(f\"üîç Coverage: {eval_results['coverage_score']:.1%}\")\n",
    "print(f\"‚ö° F1-Score: {eval_results['multilingual_f1']:.3f}\")\n",
    "print(f\"üìè Avg Similarity: {eval_results['average_similarity_score']:.3f}\")\n",
    "        \n",
    "# Provide interpretation\n",
    "print(\"\\n=== Model Quality Interpretation ===\")\n",
    "if eval_results['partial_match_accuracy'] >= 0.8:\n",
    "    print(\"‚úÖ EXCELLENT: Model performs very well!\")\n",
    "elif eval_results['partial_match_accuracy'] >= 0.6:\n",
    "    print(\"‚úÖ GOOD: Model performs reasonably well\")\n",
    "elif eval_results['partial_match_accuracy'] >= 0.4:\n",
    "    print(\"‚ö†Ô∏è FAIR: Model needs improvement\")\n",
    "else:\n",
    "    print(\"‚ùå POOR: Model needs significant work\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n=== Recommendations ===\")\n",
    "if eval_results['coverage_score'] < 0.9:\n",
    "    print(\"‚Ä¢ Consider expanding vocabulary coverage\")\n",
    "if eval_results['exact_match_accuracy'] < 0.3:\n",
    "    print(\"‚Ä¢ Multiple meanings handling could be improved\")\n",
    "if eval_results['multilingual_f1'] < 0.5:\n",
    "    print(\"‚Ä¢ Consider more sophisticated NLP techniques\")\n",
    "else:\n",
    "    print(\"Model is performing well. No recommendations provided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59181c3e",
   "metadata": {},
   "source": [
    "**Cross Validating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94170bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(self, k_folds=5, random_state=42):\n",
    "        \"\"\"\n",
    "        Perform k-fold cross validation\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== {k_folds}-Fold Cross Validation ===\")\n",
    "        \n",
    "        all_cree_words = list(self.cree_to_english.keys())\n",
    "        np.random.seed(random_state)\n",
    "        np.random.shuffle(all_cree_words)\n",
    "        \n",
    "        # Split into k folds\n",
    "        fold_size = len(all_cree_words) // k_folds\n",
    "        cv_results = []\n",
    "        \n",
    "        for fold in range(k_folds):\n",
    "            print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "            \n",
    "            # Define test set for this fold\n",
    "            start_idx = fold * fold_size\n",
    "            end_idx = start_idx + fold_size if fold < k_folds - 1 else len(all_cree_words)\n",
    "            test_words = all_cree_words[start_idx:end_idx]\n",
    "            \n",
    "            # Evaluate on this fold\n",
    "            exact_matches = 0\n",
    "            partial_matches = 0\n",
    "            similarity_scores = []\n",
    "            \n",
    "            for cree_word in test_words:\n",
    "                true_meanings = set(self.cree_to_english[cree_word])\n",
    "                predicted_meanings = self.find_translations(cree_word, top_k=5)\n",
    "                predicted_set = set(predicted_meanings)\n",
    "                \n",
    "                if true_meanings == predicted_set:\n",
    "                    exact_matches += 1\n",
    "                \n",
    "                if len(true_meanings.intersection(predicted_set)) > 0:\n",
    "                    partial_matches += 1\n",
    "                \n",
    "                # Jaccard similarity\n",
    "                if len(predicted_meanings) > 0:\n",
    "                    jaccard_sim = len(true_meanings.intersection(predicted_set)) / len(true_meanings.union(predicted_set))\n",
    "                    similarity_scores.append(jaccard_sim)\n",
    "                else:\n",
    "                    similarity_scores.append(0)\n",
    "            \n",
    "            fold_results = {\n",
    "                'fold': fold + 1,\n",
    "                'exact_accuracy': exact_matches / len(test_words),\n",
    "                'partial_accuracy': partial_matches / len(test_words),\n",
    "                'avg_similarity': np.mean(similarity_scores)\n",
    "            }\n",
    "            \n",
    "            cv_results.append(fold_results)\n",
    "            print(f\"  Exact Accuracy: {fold_results['exact_accuracy']:.3f}\")\n",
    "            print(f\"  Partial Accuracy: {fold_results['partial_accuracy']:.3f}\")\n",
    "            print(f\"  Avg Similarity: {fold_results['avg_similarity']:.3f}\")\n",
    "        \n",
    "        # Calculate overall CV results\n",
    "        cv_summary = {\n",
    "            'mean_exact_accuracy': np.mean([r['exact_accuracy'] for r in cv_results]),\n",
    "            'std_exact_accuracy': np.std([r['exact_accuracy'] for r in cv_results]),\n",
    "            'mean_partial_accuracy': np.mean([r['partial_accuracy'] for r in cv_results]),\n",
    "            'std_partial_accuracy': np.std([r['partial_accuracy'] for r in cv_results]),\n",
    "            'mean_similarity': np.mean([r['avg_similarity'] for r in cv_results]),\n",
    "            'std_similarity': np.std([r['avg_similarity'] for r in cv_results])\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n--- Cross Validation Summary ---\")\n",
    "        print(f\"Exact Accuracy: {cv_summary['mean_exact_accuracy']:.3f} ¬± {cv_summary['std_exact_accuracy']:.3f}\")\n",
    "        print(f\"Partial Accuracy: {cv_summary['mean_partial_accuracy']:.3f} ¬± {cv_summary['std_partial_accuracy']:.3f}\")\n",
    "        print(f\"Similarity Score: {cv_summary['mean_similarity']:.3f} ¬± {cv_summary['std_similarity']:.3f}\")\n",
    "        \n",
    "        return cv_results, cv_summary\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.cross_validate_model = cross_validate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c61c35f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 5-Fold Cross Validation ===\n",
      "\n",
      "Fold 1/5\n",
      "  Exact Accuracy: 1.000\n",
      "  Partial Accuracy: 1.000\n",
      "  Avg Similarity: 1.000\n",
      "\n",
      "Fold 2/5\n",
      "  Exact Accuracy: 1.000\n",
      "  Partial Accuracy: 1.000\n",
      "  Avg Similarity: 1.000\n",
      "\n",
      "Fold 3/5\n",
      "  Exact Accuracy: 1.000\n",
      "  Partial Accuracy: 1.000\n",
      "  Avg Similarity: 1.000\n",
      "\n",
      "Fold 4/5\n",
      "  Exact Accuracy: 1.000\n",
      "  Partial Accuracy: 1.000\n",
      "  Avg Similarity: 1.000\n",
      "\n",
      "Fold 5/5\n",
      "  Exact Accuracy: 1.000\n",
      "  Partial Accuracy: 1.000\n",
      "  Avg Similarity: 1.000\n",
      "\n",
      "--- Cross Validation Summary ---\n",
      "Exact Accuracy: 1.000 ¬± 0.000\n",
      "Partial Accuracy: 1.000 ¬± 0.000\n",
      "Similarity Score: 1.000 ¬± 0.000\n"
     ]
    }
   ],
   "source": [
    "cv_results, cv_summary = model.cross_validate_model(k_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3315bf",
   "metadata": {},
   "source": [
    "**Learning Curve Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b90efba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze how model performance changes with dataset size\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Learning Curve Analysis ===\")\n",
    "        \n",
    "        all_words = list(self.cree_to_english.keys())\n",
    "        dataset_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        \n",
    "        learning_curve_results = []\n",
    "        \n",
    "        for size in dataset_sizes:\n",
    "            # Sample subset of data\n",
    "            n_words = int(len(all_words) * size)\n",
    "            sampled_words = np.random.choice(all_words, n_words, replace=False)\n",
    "            \n",
    "            # Create temporary model with subset\n",
    "            temp_cree_to_english = {word: self.cree_to_english[word] for word in sampled_words}\n",
    "            \n",
    "            # Test on remaining data\n",
    "            test_words = [word for word in all_words if word not in sampled_words]\n",
    "            if len(test_words) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Evaluate\n",
    "            partial_matches = 0\n",
    "            for test_word in test_words[:min(50, len(test_words))]:  # Limit for efficiency\n",
    "                true_meanings = set(self.cree_to_english[test_word])\n",
    "                predicted_meanings = self.find_translations(test_word, top_k=3)\n",
    "                predicted_set = set(predicted_meanings)\n",
    "                \n",
    "                if len(true_meanings.intersection(predicted_set)) > 0:\n",
    "                    partial_matches += 1\n",
    "            \n",
    "            accuracy = partial_matches / min(50, len(test_words))\n",
    "            \n",
    "            learning_curve_results.append({\n",
    "                'dataset_size': size,\n",
    "                'n_training_words': n_words,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "            \n",
    "            print(f\"Dataset size: {size:.1f} ({n_words} words) -> Accuracy: {accuracy:.3f}\")\n",
    "        \n",
    "        return learning_curve_results\n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.learning_curve_analysis = learning_curve_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad2f3ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Learning Curve Analysis ===\n",
      "Dataset size: 0.1 (99 words) -> Accuracy: 1.000\n",
      "Dataset size: 0.2 (199 words) -> Accuracy: 1.000\n",
      "Dataset size: 0.3 (299 words) -> Accuracy: 1.000\n",
      "Dataset size: 0.4 (399 words) -> Accuracy: 1.000\n",
      "Dataset size: 0.5 (499 words) -> Accuracy: 1.000\n",
      "Dataset size: 0.6 (599 words) -> Accuracy: 1.000\n",
      "Dataset size: 0.7 (699 words) -> Accuracy: 1.000\n",
      "Dataset size: 0.8 (799 words) -> Accuracy: 1.000\n",
      "Dataset size: 0.9 (899 words) -> Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "learning_curve_data = model.learning_curve_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c0198",
   "metadata": {},
   "source": [
    "**Model Confidence Score Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a955d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_confidence_score(self, cree_word, threshold=0.3):\n",
    "        \"\"\"\n",
    "        Calculate confidence score for a prediction\n",
    "        \"\"\"\n",
    "        predictions = self.find_translations(cree_word, top_k=5)\n",
    "        \n",
    "        if not predictions:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate confidence based on similarity scores\n",
    "        unique_cree = list(self.cree_to_english.keys())\n",
    "        \n",
    "        if cree_word.lower() in unique_cree:\n",
    "            # Direct match - high confidence\n",
    "            return 1.0\n",
    "        else:\n",
    "            # Similarity-based match\n",
    "            query_embedding = self.vectorizer.transform([cree_word.lower()])\n",
    "            similarities = cosine_similarity(query_embedding, self.cree_embeddings)[0]\n",
    "            max_similarity = np.max(similarities)\n",
    "            \n",
    "            # Normalize confidence score\n",
    "            confidence = min(max_similarity / threshold, 1.0) if max_similarity > 0 else 0.0\n",
    "            return confidence\n",
    "        \n",
    "\n",
    "# Attach it to the class\n",
    "CreeLearningModel.model_confidence_score = model_confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a719dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Confidence Scoring Examples ===\n",
      "\n",
      "'ahƒìw' -> Confidence: 1.000, Translations: ['he placed him']\n",
      "'wƒÅsaskotƒìnikan' -> Confidence: 1.000, Translations: ['a lamp', 'lightbulb']\n",
      "'ahin' -> Confidence: 1.000, Translations: ['put', 'place me']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Confidence Scoring Examples ===\\n\")\n",
    "confidence_test_words = ['ahƒìw', 'wƒÅsaskotƒìnikan', 'ahin']\n",
    "for word in confidence_test_words:\n",
    "    confidence = model.model_confidence_score(word)\n",
    "    translations = model.find_translations(word)\n",
    "    print(f\"'{word}' -> Confidence: {confidence:.3f}, Translations: {translations}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6e49f",
   "metadata": {},
   "source": [
    "**Model Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "667128ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Statistics ===\n",
      "Total Cree words: 999\n",
      "Total English meanings: 1099\n",
      "Cree words with multiple meanings: 147\n",
      "Average meanings per Cree word: 1.19\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Model Statistics ===\")\n",
    "print(f\"Total Cree words: {len(model.cree_to_english)}\")\n",
    "print(f\"Total English meanings: {len(model.english_to_cree)}\")\n",
    "        \n",
    "multi_meaning = sum(1 for v in model.cree_to_english.values() if len(v) > 1)\n",
    "print(f\"Cree words with multiple meanings: {multi_meaning}\")\n",
    "        \n",
    "avg_meanings = np.mean([len(v) for v in model.cree_to_english.values()])\n",
    "print(f\"Average meanings per Cree word: {avg_meanings:.2f}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
